--- ./setup.py	(original)
+++ ./setup.py	(refactored)
@@ -9,7 +9,7 @@
 		install_lib.install_lib.run(self)
 		for fn in self.get_outputs():
 			if fn.split('/')[-2] == 'bin':
-				mode = ((os.stat(fn).st_mode) | 0555) & 07777
+				mode = ((os.stat(fn).st_mode) | 0o555) & 0o7777
 				log.info("changing mode of %s to %o", fn, mode)
 				os.chmod(fn, mode)
 
--- ./microbe_census/microbe_census.py	(original)
+++ ./microbe_census/microbe_census.py	(refactored)
@@ -123,7 +123,7 @@
 	
 def check_paths(paths):
 	""" Check that all relative paths exist """
-	for my_path in paths.values():
+	for my_path in list(paths.values()):
 		if os.path.isfile(my_path):
 			continue
 		elif os.path.isdir(my_path):
@@ -240,23 +240,23 @@
 def print_copyright():
 	# print out copyright information
 	print ("\nMicrobeCensus - estimation of average genome size from shotgun sequence data")
-	print ("version %s; github.com/snayfach/MicrobeCensus" % __version__)
+	print(("version %s; github.com/snayfach/MicrobeCensus" % __version__))
 	print ("Copyright (C) 2013-2015 Stephen Nayfach")
 	print ("Freely distributed under the GNU General Public License (GPLv3)\n")
 
 def print_parameters(args):
 	# print out parameters
 	print ("=============Parameters==============")
-	print ("Input metagenome: %s" % args['seqfiles'])
-	print ("Output file: %s" % args['outfile'])
-	print ("Reads trimmed to: %s bp" % args['read_length'])
-	print ("Maximum reads sampled: %s" % args['nreads'])
-	print ("Threads to use for db search: %s" % args['threads'])
-	print ("Minimum base-level quality score: %s" % (args['min_quality'] if args['file_type'] == 'fastq' else 'NA'))
-	print ("Minimum read-level quality score: %s" % (args['mean_quality'] if args['file_type'] == 'fastq' else 'NA'))
-	print ("Maximum percent unknown bases/read: %s" % args['max_unknown'])
-	print ("Filter duplicate reads: %s" % args['filter_dups'])
-	print ("Keep temporary files: %s\n" % args['keep_tmp'])
+	print(("Input metagenome: %s" % args['seqfiles']))
+	print(("Output file: %s" % args['outfile']))
+	print(("Reads trimmed to: %s bp" % args['read_length']))
+	print(("Maximum reads sampled: %s" % args['nreads']))
+	print(("Threads to use for db search: %s" % args['threads']))
+	print(("Minimum base-level quality score: %s" % (args['min_quality'] if args['file_type'] == 'fastq' else 'NA')))
+	print(("Minimum read-level quality score: %s" % (args['mean_quality'] if args['file_type'] == 'fastq' else 'NA')))
+	print(("Maximum percent unknown bases/read: %s" % args['max_unknown']))
+	print(("Filter duplicate reads: %s" % args['filter_dups']))
+	print(("Keep temporary files: %s\n" % args['keep_tmp']))
 
 def quality_filter(rec, args):
 	""" Return true if read fails QC """
@@ -362,10 +362,10 @@
 	else:
 		args['sampled_reads'] = read_id
 	if args['verbose']:
-		print ("\t%s reads shorter than %s bp and skipped" % (too_short, args['read_length']))
-		print ("\t%s low quality reads found and skipped" % low_qual)
-		print ("\t%s duplicate reads found and skipped" % dups)
-		print ("\t%s reads sampled from seqfile" % read_id)
+		print(("\t%s reads shorter than %s bp and skipped" % (too_short, args['read_length'])))
+		print(("\t%s low quality reads found and skipped" % low_qual))
+		print(("\t%s duplicate reads found and skipped" % dups))
+		print(("\t%s reads sampled from seqfile" % read_id))
 
 def search_seqs(args, paths):
 	""" Search high quality reads against marker genes using RAPsearch2 """
@@ -383,7 +383,7 @@
 			if line[0] != '#': hits.append(line.split()[0])
 		distinct_hits = len(set(hits))
 		if args['verbose']:
-			print ("\t%s reads hit marker proteins" % str(distinct_hits))
+			print(("\t%s reads hit marker proteins" % str(distinct_hits)))
 	else:
 		clean_up(paths)
 		sys.exit("\nDatabase search has exited with the following error:\n%s" % error)
@@ -455,14 +455,14 @@
 		clean_up(paths)
 		sys.exit("\nError: No hits to marker proteins - cannot estimate genome size! Rerun program with more reads.")
 	if args['verbose']:
-		print ("\t%s reads assigned to a marker protein" % len(best_hits))
+		print(("\t%s reads assigned to a marker protein" % len(best_hits)))
 	return best_hits
 
 def aggregate_hits(args, paths, best_hits):
 	""" Count hits to each gene family from and return dictionary of results {fam_id:hits} """
 	optpars = find_opt_pars(paths['params'], args['read_length'])
 	agg_hits = {}
-	for fam_id, aln, cov, score in best_hits.values():
+	for fam_id, aln, cov, score in list(best_hits.values()):
 		aln_stat = optpars[fam_id]['aln_stat']
 		if fam_id not in agg_hits:
 			agg_hits[fam_id] = 1.0 if aln_stat == 'hits' else cov if aln_stat == 'cov' else aln
@@ -487,7 +487,7 @@
 	weights = read_dic(paths['weights'], header=False, dtype='float')
 	# predict average genome size for each marker
 	estimates = {}
-	for fam_id, hits in agg_hits.items():
+	for fam_id, hits in list(agg_hits.items()):
 		coeff = coeffs['_'.join([str(args['read_length']),fam_id])]
 		rate = hits/(args['sampled_reads'] * args['read_length'])
 		if rate == 0: continue
@@ -497,7 +497,7 @@
 	mad_estimate = mad(list(estimates.values()))
 	median_estimate = median(list(estimates.values()))
 	sum_weights = 0
-	for fam_id, estimate in estimates.items():
+	for fam_id, estimate in list(estimates.items()):
 		if abs(estimate - median_estimate) >= mad_estimate:
 			continue
 		else:
@@ -507,7 +507,7 @@
 	est_ags = est_ags/sum_weights
 	# report results
 	if args['verbose']:
-		print ("\t%s bp" % str(round(est_ags, 2)))
+		print(("\t%s bp" % str(round(est_ags, 2))))
 	return est_ags
 
 def report_results(args, est_ags, count_bases):
--- ./tests/test_microbe_census.py	(original)
+++ ./tests/test_microbe_census.py	(refactored)
@@ -28,7 +28,7 @@
 	""" check whether file is read into list properly """
 	
 	def setUp(self): # create test files in tmp directory
-		self.exp_values = range(10)
+		self.exp_values = list(range(10))
 		self.dir = tempfile.mkdtemp()
 		self.inpath = os.path.join(self.dir, 'tmp.txt')
 		for i in range(10):
--- ./training/class_reads.py	(original)
+++ ./training/class_reads.py	(refactored)
@@ -67,20 +67,20 @@
 			args_list.append(args)
 
 # Print arguments
-print "Threads to use: %s" % threads
-print "Number of search results to classify: %s" % len(args_list)
-print "Output directory: %s" % hits_dir
+print("Threads to use: %s" % threads)
+print("Number of search results to classify: %s" % len(args_list))
+print("Output directory: %s" % hits_dir)
 
 # Classify reads in parallel
 parallel_function(classify_reads, args_list, threads)
 
 # Write gene_fam.map and gene_len.map
 f_out = open(gene_fam_path, 'w')
-for gene, fam in gene2fam.iteritems():
+for gene, fam in gene2fam.items():
 	record = [str(gene), str(fam)]
 	f_out.write('\t'.join(record)+'\n')
 f_out = open(gene_len_path, 'w')
-for gene, length in gene2len.iteritems():
+for gene, length in gene2len.items():
 	record = [str(gene), str(length)]
 	f_out.write('\t'.join(record)+'\n')
 
--- ./training/optimize_parameters.py	(original)
+++ ./training/optimize_parameters.py	(refactored)
@@ -30,12 +30,12 @@
 preds_path  = os.path.join(main_dir, 'training/output/training_preds.map')
 
 # Store genome & library sizes
-print "Storing genome sizes and library sizes..."
+print("Storing genome sizes and library sizes...")
 genome2size = genome_sizes(genomes_dir)
 library2size = library_sizes(reads_dir)
 
 # Store classification rates
-print "Storing classification rates..."
+print("Storing classification rates...")
 class_rates = store_rates(hits_dir, library2size)
 
 # Build argument list
@@ -55,10 +55,10 @@
 # Compute x-validation error for each set of parameters
 # and find optimal parameters
 
-print "Evaluating performance for %s parameters..." % len(args_list)
+print("Evaluating performance for %s parameters..." % len(args_list))
 xval_error = parallel_return_function(xvalidation, args_list, threads)
 
-print "Finding optimal parameters..."
+print("Finding optimal parameters...")
 opt_pars = find_opt_pars(xval_error)
 #	write results
 f_out = open(pars_path, 'w')
@@ -70,7 +70,7 @@
 	record = [ str(x) for x in [fam, read_length, aln_cov, max_pid, min_score, aln_stat] ]
 	f_out.write('\t'.join(record)+'\n')
 
-print "Finding proportionality constants..."
+print("Finding proportionality constants...")
 prop_consts = {}
 for read_length, fam in sorted(opt_pars.keys()):
 	min_score, max_pid, aln_cov, rate_type = opt_pars[(read_length, fam)]['pars']
@@ -85,7 +85,7 @@
 	record = [read_length + '_' + fam, str(prop_const)]
 	f_out.write('\t'.join(record)+'\n')
 
-print "Getting per-gene AGS predictions..."
+print("Getting per-gene AGS predictions...")
 training_ests = {}
 for read_length, fam in sorted(opt_pars.keys()):
 	min_score, max_pid, aln_cov, rate_type = opt_pars[(read_length, fam)]['pars']
--- ./training/search_reads.py	(original)
+++ ./training/search_reads.py	(refactored)
@@ -36,7 +36,7 @@
 
 # Build database if necessary
 if not os.path.isfile(rapdb_path):
-	print 'RAPsearch2 database does not exist. Creating...'
+	print('RAPsearch2 database does not exist. Creating...')
 	# cat sequences together in tempfile
 	f_out = open(seqs_path, 'w')
 	gene_fams = os.listdir(gene_fam_dir)
@@ -69,10 +69,10 @@
 			args_list.append(args)
 
 # Print arguments
-print "Threads to use: %s" % threads
-print "Read lengths: %s" % os.listdir(reads_dir)
-print "Number of libraries to search: %s" % len(args_list)
-print "Output directory: %s" % search_dir
+print("Threads to use: %s" % threads)
+print("Read lengths: %s" % os.listdir(reads_dir))
+print("Number of libraries to search: %s" % len(args_list))
+print("Output directory: %s" % search_dir)
 
 # Search libraries in parallel
 parallel_subprocess(command, args_list, threads)
--- ./training/seq_sim.py	(original)
+++ ./training/seq_sim.py	(refactored)
@@ -8,22 +8,22 @@
 try:
     import sys
 except Exception:
-    print 'Module "sys" not installed'; exit()
+    print('Module "sys" not installed'); exit()
 
 try:
     import os
 except Exception:
-    print 'Module "os" not installed'; exit()
+    print('Module "os" not installed'); exit()
     
 try:
     import optparse
 except Exception:
-    print 'Module "optparse" not installed'; exit()
+    print('Module "optparse" not installed'); exit()
 
 try:
     import datetime
 except Exception:
-    print 'Module "datetime" not installed'; exit()
+    print('Module "datetime" not installed'); exit()
 
 from sim_functions import *
 
@@ -48,10 +48,10 @@
 	paired_end = options.paired_end
 	error_model = options.error_model
 	error_rate = options.error_rate
-except Exception, e:
-    print "Incorrect number of command line arguments."
-    print "\nUsage: python seq_sim.py [-options] <seqfile> <outfile>"
-    print "For all options enter: python seq_sim.py -h"
+except Exception as e:
+    print("Incorrect number of command line arguments.")
+    print("\nUsage: python seq_sim.py [-options] <seqfile> <outfile>")
+    print("For all options enter: python seq_sim.py -h")
     sys.exit()
 
 # check for argument validity
--- ./training/sim_functions.py	(original)
+++ ./training/sim_functions.py	(refactored)
@@ -6,27 +6,27 @@
 try:
     import random
 except Exception:
-    print 'Module "random" not installed'; exit()
+    print('Module "random" not installed'); exit()
 
 try:
     import gzip
 except Exception:
-    print 'Module "gzip" not installed'; exit()
+    print('Module "gzip" not installed'); exit()
 
 try:
     import os
 except Exception:
-    print 'Module "os" not installed'; exit()
+    print('Module "os" not installed'); exit()
 
 try:
     import Bio.SeqIO
 except Exception:
-    print 'Module "Bio.SeqIO" not installed'; exit()
+    print('Module "Bio.SeqIO" not installed'); exit()
     
 try:
     import math
 except Exception:
-    print 'Module "math" not installed'; exit()
+    print('Module "math" not installed'); exit()
 
 
 #   define functions
@@ -38,7 +38,7 @@
 def parse_genome(genome):
     scaffolds = []
     lengths = []
-    for scaffold in genome.keys():
+    for scaffold in list(genome.keys()):
         scaffolds.append(scaffold)
         lengths.append(len(genome[scaffold]))
     return scaffolds, [length/float(sum(lengths)) for length in lengths], sum(lengths)
@@ -64,7 +64,7 @@
     return ''.join([comp[base] for base in seq[::-1]])
 
 def amplify_frag(frag_seq, gc_to_bias):
-    gc = find_nearest(compute_gcc(frag_seq), gc_to_bias.keys())
+    gc = find_nearest(compute_gcc(frag_seq), list(gc_to_bias.keys()))
     return random.random() < gc_to_bias[gc]
 
 def compute_gcc(seq):
--- ./training/sim_reads.py	(original)
+++ ./training/sim_reads.py	(refactored)
@@ -55,11 +55,11 @@
 		args_list.append(args)
 
 # Print arguments
-print "Threads to use: %s" % threads
-print "Read length: %s" % read_length
-print "Genome coverage: %s" % coverage
-print "Number of libraries: %s" % len(args_list)
-print "Output directory: %s" % reads_dir
+print("Threads to use: %s" % threads)
+print("Read length: %s" % read_length)
+print("Genome coverage: %s" % coverage)
+print("Number of libraries: %s" % len(args_list))
+print("Output directory: %s" % reads_dir)
 
 # Simulate libraries in parallel
 parallel_subprocess(command, args_list, threads)
--- ./training/training.py	(original)
+++ ./training/training.py	(refactored)
@@ -116,8 +116,8 @@
 	fold_size = n/x
 	test_start = fold_size * i - fold_size
 	test_stop = test_start + fold_size - 1
-	test_indexes = range(test_start, test_stop + 1)
-	train_indexes = range(n)
+	test_indexes = list(range(test_start, test_stop + 1))
+	train_indexes = list(range(n))
 	for j in test_indexes:
 		train_indexes.remove(j)
 	return train_indexes, test_indexes
@@ -294,7 +294,7 @@
 			best_hits[read_id] = hit
 		elif best_hits[read_id][-1] < score:
 			best_hits[read_id] = hit
-	return best_hits.values()
+	return list(best_hits.values())
 
 def aggregate_hits(hits, fams, gene2len):
 	""" Aggregate hits to each gene family """
@@ -328,7 +328,7 @@
 				# find best hits
 				best_hits = find_best_hits(hits_score_filt)
 				# count hits to each marker familes
-				for fam, stats in aggregate_hits(best_hits, fams, gene2len).iteritems():
+				for fam, stats in aggregate_hits(best_hits, fams, gene2len).items():
 					record = [str(x) for x in [fam, aln_cov, max_pid, min_score, stats['hits'], stats['aln'], stats['cov']]]
 					# write results to p_out
 					f_out.write('\t'.join(record)+'\n')
